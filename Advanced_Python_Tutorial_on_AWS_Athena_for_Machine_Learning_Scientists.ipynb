{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+l7iZ1iCYmmO3nepklLEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jman4162/aws-for-ml/blob/main/Advanced_Python_Tutorial_on_AWS_Athena_for_Machine_Learning_Scientists.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Python Tutorial on AWS Athena for Machine Learning Scientists\n",
        "\n",
        "Name: John Hodge\n",
        "Date: 09/23/24\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Prerequisites](#prerequisites)\n",
        "3. [Setting Up AWS Athena](#setting-up-aws-athena)\n",
        "4. [Configuring Your Python Environment](#configuring-your-python-environment)\n",
        "5. [Using Boto3 to Interact with Athena](#using-boto3-to-interact-with-athena)\n",
        "    - [Establishing a Connection](#establishing-a-connection)\n",
        "    - [Executing Queries](#executing-queries)\n",
        "    - [Fetching Query Results](#fetching-query-results)\n",
        "6. [Using PandasCursor for Simplified Data Retrieval](#using-pandascursor-for-simplified-data-retrieval)\n",
        "7. [Integrating Athena Queries into Machine Learning Workflows](#integrating-athena-queries-into-machine-learning-workflows)\n",
        "8. [Optimizing Queries and Performance](#optimizing-queries-and-performance)\n",
        "9. [Security Best Practices](#security-best-practices)\n",
        "10. [Advanced Topics](#advanced-topics)\n",
        "    - [Partitioning and Compression](#partitioning-and-compression)\n",
        "    - [Using Presto SQL Features](#using-presto-sql-features)\n",
        "    - [Automating Workflows with AWS Lambda](#automating-workflows-with-aws-lambda)\n",
        "11. [Conclusion](#conclusion)\n",
        "12. [References](#references)\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "AWS Athena is a serverless, interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. For machine learning scientists, Athena provides a powerful tool to preprocess and analyze large datasets without the overhead of managing infrastructure. This tutorial dives deep into leveraging Python, specifically using Boto3 and PandasCursor, to interact with Athena efficiently, integrating it seamlessly into machine learning pipelines.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before diving into the tutorial, ensure you have the following:\n",
        "\n",
        "- **AWS Account**: Access to AWS services, specifically Athena and S3.\n",
        "- **IAM Permissions**: Appropriate permissions to use Athena, S3, and other related services.\n",
        "- **Python Environment**: Python 3.7 or later installed.\n",
        "- **AWS CLI**: Configured with your credentials.\n",
        "- **Basic Knowledge**: Familiarity with Python, SQL, and machine learning concepts.\n",
        "\n",
        "## Setting Up AWS Athena\n",
        "\n",
        "1. **Create an S3 Bucket for Query Results**:\n",
        "   \n",
        "   Athena stores query results in an S3 bucket. If you donâ€™t have one:\n",
        "\n",
        "   - Navigate to the [S3 Console](https://console.aws.amazon.com/s3/).\n",
        "   - Click on **Create bucket**.\n",
        "   - Provide a unique bucket name and select the desired region.\n",
        "   - Configure settings as needed and create the bucket.\n",
        "\n",
        "2. **Configure Athena**:\n",
        "\n",
        "   - Go to the [Athena Console](https://console.aws.amazon.com/athena/).\n",
        "   - On the first launch, you'll be prompted to set the query result location. Choose the S3 bucket you created.\n",
        "   \n",
        "3. **Prepare Your Data**:\n",
        "\n",
        "   Ensure your data is stored in Amazon S3 in a format supported by Athena (e.g., CSV, JSON, Parquet, ORC).\n",
        "\n",
        "4. **Define a Database and Tables**:\n",
        "\n",
        "   Use the Athena console or SQL scripts to define databases and tables that map to your S3 data.\n",
        "\n",
        "   ```sql\n",
        "   CREATE DATABASE IF NOT EXISTS ml_database;\n",
        "\n",
        "   CREATE EXTERNAL TABLE IF NOT EXISTS ml_database.sample_data (\n",
        "       id INT,\n",
        "       feature1 DOUBLE,\n",
        "       feature2 STRING,\n",
        "       label INT\n",
        "   )\n",
        "   ROW FORMAT DELIMITED\n",
        "   FIELDS TERMINATED BY ','\n",
        "   STORED AS TEXTFILE\n",
        "   LOCATION 's3://your-bucket/path/to/data/';\n",
        "   ```\n",
        "\n",
        "## Configuring Your Python Environment\n",
        "\n",
        "1. **Install Required Libraries**:\n",
        "\n",
        "   ```bash\n",
        "   pip install boto3 pandas pyathena\n",
        "   ```\n",
        "\n",
        "2. **Set Up AWS Credentials**:\n",
        "\n",
        "   Ensure your AWS credentials are configured. You can set them up using the AWS CLI:\n",
        "\n",
        "   ```bash\n",
        "   aws configure\n",
        "   ```\n",
        "\n",
        "   Alternatively, set environment variables:\n",
        "\n",
        "   ```bash\n",
        "   export AWS_ACCESS_KEY_ID='your_access_key'\n",
        "   export AWS_SECRET_ACCESS_KEY='your_secret_key'\n",
        "   export AWS_DEFAULT_REGION='your_region'\n",
        "   ```\n",
        "\n",
        "## Using Boto3 to Interact with Athena\n",
        "\n",
        "Boto3 is the AWS SDK for Python, allowing you to interact with Athena programmatically.\n",
        "\n",
        "### Establishing a Connection\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "import time\n",
        "\n",
        "# Initialize the Athena client\n",
        "athena_client = boto3.client('athena', region_name='your-region')\n",
        "\n",
        "# Define your S3 output bucket\n",
        "OUTPUT_BUCKET = 's3://your-output-bucket/path/'\n",
        "```\n",
        "\n",
        "### Executing Queries\n",
        "\n",
        "```python\n",
        "def execute_query(query, database='ml_database'):\n",
        "    response = athena_client.start_query_execution(\n",
        "        QueryString=query,\n",
        "        QueryExecutionContext={'Database': database},\n",
        "        ResultConfiguration={'OutputLocation': OUTPUT_BUCKET}\n",
        "    )\n",
        "    return response['QueryExecutionId']\n",
        "```\n",
        "\n",
        "### Fetching Query Results\n",
        "\n",
        "```python\n",
        "def get_query_results(query_execution_id):\n",
        "    # Wait for the query to complete\n",
        "    while True:\n",
        "        response = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
        "        status = response['QueryExecution']['Status']['State']\n",
        "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
        "            break\n",
        "        print(\"Waiting for query to complete...\")\n",
        "        time.sleep(2)\n",
        "    \n",
        "    if status == 'SUCCEEDED':\n",
        "        results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
        "        return results\n",
        "    else:\n",
        "        raise Exception(f\"Query {status}\")\n",
        "```\n",
        "\n",
        "### Complete Example\n",
        "\n",
        "```python\n",
        "query = \"\"\"\n",
        "SELECT feature1, feature2, label\n",
        "FROM ml_database.sample_data\n",
        "WHERE label IS NOT NULL\n",
        "LIMIT 1000\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query\n",
        "query_execution_id = execute_query(query)\n",
        "\n",
        "# Fetch the results\n",
        "results = get_query_results(query_execution_id)\n",
        "\n",
        "# Process results\n",
        "rows = results['ResultSet']['Rows']\n",
        "header = [col['VarCharValue'] for col in rows[0]['Data']]\n",
        "data = [dict(zip(header, [col.get('VarCharValue') for col in row['Data']])) for row in rows[1:]]\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "```\n",
        "\n",
        "**Note**: While Boto3 provides granular control, handling pagination and data transformation can be cumbersome for large datasets.\n",
        "\n",
        "## Using PandasCursor for Simplified Data Retrieval\n",
        "\n",
        "[PandasCursor](https://github.com/laughingman7743/PyAthena) is a convenient interface that allows you to execute Athena queries and fetch results directly into pandas DataFrames.\n",
        "\n",
        "1. **Install PyAthena**:\n",
        "\n",
        "   ```bash\n",
        "   pip install PyAthena[pandas]\n",
        "   ```\n",
        "\n",
        "2. **Using PandasCursor**:\n",
        "\n",
        "   ```python\n",
        "   from pyathena import connect\n",
        "   import pandas as pd\n",
        "\n",
        "   # Establish connection\n",
        "   conn = connect(\n",
        "       s3_staging_dir=OUTPUT_BUCKET,\n",
        "       region_name='your-region',\n",
        "       aws_access_key_id='your_access_key',\n",
        "       aws_secret_access_key='your_secret_key'\n",
        "   )\n",
        "\n",
        "   # Execute query and fetch into DataFrame\n",
        "   query = \"\"\"\n",
        "   SELECT feature1, feature2, label\n",
        "   FROM ml_database.sample_data\n",
        "   WHERE label IS NOT NULL\n",
        "   LIMIT 1000\n",
        "   \"\"\"\n",
        "\n",
        "   df = pd.read_sql(query, conn)\n",
        "   print(df.head())\n",
        "   ```\n",
        "\n",
        "**Advantages of PandasCursor**:\n",
        "\n",
        "- **Simplicity**: Directly fetch data into pandas DataFrames.\n",
        "- **Performance**: Efficient handling of large datasets with support for pagination.\n",
        "- **Integration**: Seamlessly integrates with pandas-based machine learning workflows.\n",
        "\n",
        "## Integrating Athena Queries into Machine Learning Workflows\n",
        "\n",
        "Once data is retrieved from Athena, it can be directly used for machine learning tasks. Here's how you can integrate Athena queries into a typical ML pipeline:\n",
        "\n",
        "1. **Data Retrieval**:\n",
        "\n",
        "   Use Boto3 or PandasCursor to fetch the required data.\n",
        "\n",
        "   ```python\n",
        "   # Using PyAthena\n",
        "   df = pd.read_sql(query, conn)\n",
        "   ```\n",
        "\n",
        "2. **Data Preprocessing**:\n",
        "\n",
        "   Clean and preprocess the data as required.\n",
        "\n",
        "   ```python\n",
        "   # Handle missing values\n",
        "   df.fillna(0, inplace=True)\n",
        "\n",
        "   # Encode categorical variables\n",
        "   df['feature2'] = df['feature2'].astype('category').cat.codes\n",
        "   ```\n",
        "\n",
        "3. **Feature Engineering**:\n",
        "\n",
        "   Create new features or transform existing ones to improve model performance.\n",
        "\n",
        "   ```python\n",
        "   df['feature1_squared'] = df['feature1'] ** 2\n",
        "   ```\n",
        "\n",
        "4. **Model Training**:\n",
        "\n",
        "   Use libraries like scikit-learn, TensorFlow, or PyTorch to train models.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "   X = df[['feature1', 'feature2', 'feature1_squared']]\n",
        "   y = df['label']\n",
        "\n",
        "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "   model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "   model.fit(X_train, y_train)\n",
        "\n",
        "   accuracy = model.score(X_test, y_test)\n",
        "   print(f\"Model Accuracy: {accuracy}\")\n",
        "   ```\n",
        "\n",
        "5. **Model Evaluation and Deployment**:\n",
        "\n",
        "   Evaluate model performance and deploy as needed.\n",
        "\n",
        "## Optimizing Queries and Performance\n",
        "\n",
        "Efficient querying is crucial when dealing with large datasets. Here are some strategies to optimize Athena queries:\n",
        "\n",
        "1. **Use Columnar Formats**:\n",
        "\n",
        "   Formats like Parquet or ORC reduce data size and improve query performance.\n",
        "\n",
        "   ```sql\n",
        "   CREATE EXTERNAL TABLE IF NOT EXISTS ml_database.sample_data_parquet (\n",
        "       id INT,\n",
        "       feature1 DOUBLE,\n",
        "       feature2 STRING,\n",
        "       label INT\n",
        "   )\n",
        "   STORED AS PARQUET\n",
        "   LOCATION 's3://your-bucket/path/to/parquet/';\n",
        "   ```\n",
        "\n",
        "2. **Partitioning Data**:\n",
        "\n",
        "   Partitioning allows Athena to scan only relevant data, reducing query costs and time.\n",
        "\n",
        "   ```sql\n",
        "   CREATE EXTERNAL TABLE IF NOT EXISTS ml_database.sample_data_partitioned (\n",
        "       id INT,\n",
        "       feature1 DOUBLE,\n",
        "       feature2 STRING,\n",
        "       label INT\n",
        "   )\n",
        "   PARTITIONED BY (date STRING)\n",
        "   STORED AS PARQUET\n",
        "   LOCATION 's3://your-bucket/path/to/partitioned-data/';\n",
        "   ```\n",
        "\n",
        "   Add partitions:\n",
        "\n",
        "   ```sql\n",
        "   MSCK REPAIR TABLE ml_database.sample_data_partitioned;\n",
        "   ```\n",
        "\n",
        "3. **Limit Data Scanned**:\n",
        "\n",
        "   Use specific columns in your SELECT statement instead of `SELECT *`.\n",
        "\n",
        "4. **Use Compression**:\n",
        "\n",
        "   Compress data to reduce storage size and speed up data transfer.\n",
        "\n",
        "5. **Optimize SQL Queries**:\n",
        "\n",
        "   - Avoid complex joins if possible.\n",
        "   - Use WHERE clauses to filter data early.\n",
        "   - Leverage window functions judiciously.\n",
        "\n",
        "## Security Best Practices\n",
        "\n",
        "Ensuring data security and compliance is paramount:\n",
        "\n",
        "1. **IAM Roles and Policies**:\n",
        "\n",
        "   - Use least privilege principles.\n",
        "   - Assign specific permissions to Athena and S3 resources.\n",
        "\n",
        "2. **Encrypt Data**:\n",
        "\n",
        "   - Enable server-side encryption for S3 buckets.\n",
        "   - Use HTTPS for data in transit.\n",
        "\n",
        "3. **Use Workgroups**:\n",
        "\n",
        "   Athena Workgroups help manage query usage and apply policies.\n",
        "\n",
        "4. **Audit and Monitoring**:\n",
        "\n",
        "   - Enable AWS CloudTrail for logging Athena and S3 activities.\n",
        "   - Monitor usage with Amazon CloudWatch.\n",
        "\n",
        "## Advanced Topics\n",
        "\n",
        "### Partitioning and Compression\n",
        "\n",
        "Partitioning data by commonly queried columns (e.g., date, region) can significantly enhance query performance. Combined with compression formats like Snappy or GZIP, you can achieve faster data retrieval and lower storage costs.\n",
        "\n",
        "**Example**:\n",
        "\n",
        "```sql\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS ml_database.sample_data_partitioned (\n",
        "    id INT,\n",
        "    feature1 DOUBLE,\n",
        "    feature2 STRING,\n",
        "    label INT\n",
        ")\n",
        "PARTITIONED BY (year INT, month INT, day INT)\n",
        "STORED AS PARQUET\n",
        "LOCATION 's3://your-bucket/path/to/partitioned-data/';\n",
        "```\n",
        "\n",
        "### Using Presto SQL Features\n",
        "\n",
        "Athena is powered by Presto. Leveraging Presto-specific features can enhance your querying capabilities:\n",
        "\n",
        "- **Window Functions**: Perform complex calculations across rows.\n",
        "  \n",
        "  ```sql\n",
        "  SELECT id, feature1, feature2, label,\n",
        "         AVG(feature1) OVER (PARTITION BY label) AS avg_feature1\n",
        "  FROM ml_database.sample_data;\n",
        "  ```\n",
        "\n",
        "- **Common Table Expressions (CTEs)**: Simplify complex queries.\n",
        "\n",
        "  ```sql\n",
        "  WITH filtered_data AS (\n",
        "      SELECT * FROM ml_database.sample_data\n",
        "      WHERE label IS NOT NULL\n",
        "  )\n",
        "  SELECT feature1, COUNT(*)\n",
        "  FROM filtered_data\n",
        "  GROUP BY feature1;\n",
        "  ```\n",
        "\n",
        "### Automating Workflows with AWS Lambda\n",
        "\n",
        "Automate data retrieval and preprocessing using AWS Lambda:\n",
        "\n",
        "1. **Create a Lambda Function** that executes Athena queries using Boto3.\n",
        "2. **Trigger the Function** based on events (e.g., new data arrival in S3).\n",
        "3. **Integrate with Other AWS Services** for end-to-end automation.\n",
        "\n",
        "**Example Lambda Function**:\n",
        "\n",
        "```python\n",
        "import boto3\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    athena_client = boto3.client('athena', region_name='your-region')\n",
        "    query = \"SELECT * FROM ml_database.sample_data LIMIT 1000\"\n",
        "    response = athena_client.start_query_execution(\n",
        "        QueryString=query,\n",
        "        QueryExecutionContext={'Database': 'ml_database'},\n",
        "        ResultConfiguration={'OutputLocation': 's3://your-output-bucket/path/'}\n",
        "    )\n",
        "    return {'QueryExecutionId': response['QueryExecutionId']}\n",
        "```\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "AWS Athena, combined with Python's powerful libraries like Boto3 and PandasCursor, offers a robust solution for machine learning scientists to access, preprocess, and analyze large datasets stored in Amazon S3. By following this tutorial, you've learned how to set up Athena, interact with it programmatically, optimize queries, and integrate data retrieval into your machine learning workflows securely and efficiently. Leveraging these tools and best practices will empower you to handle complex data challenges and accelerate your machine learning projects.\n",
        "\n",
        "## References\n",
        "\n",
        "- [AWS Athena Documentation](https://docs.aws.amazon.com/athena/)\n",
        "- [Boto3 Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
        "- [PyAthena GitHub Repository](https://github.com/laughingman7743/PyAthena)\n",
        "- [AWS Security Best Practices](https://aws.amazon.com/architecture/security-best-practices/)\n",
        "- [Presto SQL Documentation](https://prestodb.io/docs/current/)\n"
      ],
      "metadata": {
        "id": "m3jMaEMvtIOA"
      }
    }
  ]
}